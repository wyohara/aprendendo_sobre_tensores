# Tokeniza√ß√£o
Essa t√©cnica surgiu a partir de uma necessidade b√°sica: computadores n√£o conseguem reconhecer palavras. Ent√£o para que possamos transmitir um texto para o computador compreender precisamos criar tokens num√©ricos, onde √© poss√≠vel aplicar c√°lculos matem√°ticos e inferir dados usando estat√≠sticas ou √°lgebra linear.  
```
Texto = "Ol√°, mundo!"
Tokens = ["Ol√°", ",", "mundo", "!"]
Tokens = [15678, 11, 2345, 30]
```
Pontos importantes sobre a tokeniza√ß√£o:  
- **Uma palavra** n√£o necess√°riamente vai gerar **um token**;  
- Palavras compostas ou muito grandes podem gerar mais de um token;  
- Existem tokens especiais de marca√ß√£o para infer√™ncia, como a marca√ß√£o de in√≠cio e fim de texto, pedido do usuario, resposta do modelo entre outros.  
- O token gerado depende do contexto onde est√° a palavra e isso depende de muito treinamento.  
- Quando fornecemos **palavras desconhecidas** o tokenizador quebra a palavra em trechos com tokens conhecidos.  

## T√©cnicas de tokeniza√ß√£o
### Tokeniza√ß√£o por palavras (Word Based)
A forma mais intuitiva de tokeniza√ß√£o, onde divide o texto por palavra e pontua√ß√£o.  
- N√£o consegue lidar com palavras desconhecidas;  
- Pode ou n√£o ignorar pontua√ß√£o;  
```
"Eu amo intelig√™ncia artificial" -> ["Eu", "amo", "intelig√™ncia", "artificial", "!"]
```
### Tokeniza√ß√£o por caracteres (Character-Based)
Divide o texto em caracteres individuais.  
- Usado apenas para gerar poucas centenas de tokens;  
- capaz de lidar com todas as palavras;  
- quanto maior o texto, mais dif√≠cil o processamento;  
```
"casa" -> ["c", "a", "s", "a"]
```
### Tokeniza√ß√£o Morfol√≥gica  (Morphological)
Usa as regras lingu√≠sticas para dividir as palavras.
- Extremamente preciso
- Precisa de um conhecimento profundo do idioma
- Restrito a um idioma
- Dif√≠cil de generalizar para outros idiomas
```
"inesperadamente" ‚Üí ["in", "esper", "ada", "mente"]
```

### Tokeniza√ß√£o por Subpalavras (Subword Tokenization)
Busca o equil√≠brio entre tokeniza√ß√£o por palavra e por caractere. Possuem v√°rias formas de aplica√ß√£o:  
#### Character-Level Encoding 
- Converte os caracteres para unicode;  
- Come√ßa separando caracteres individuais e vai mesclando os caracteres a medida que verifica a frequ√äncia;
- Exige muito treinamento
- Consegue trabalhar com subpalavras e palavras desconhecidas
```
"baixo" (antes do treino ou palavra rara) -> ["b", "a", "i", "x", "o"]
"baixo" (ap√≥s treino) -> ["baix", "o"] ou ["baixo"] se for uma palavra comum
```

#### WordPiece
Semelhante ao BPE mas usa an√°lise estat√≠stica para gerar os tokens.  
- Usado por BERT, DistilBERT
- Usa o s√≠mbolo `##` para indicar continua√ß√£o de palavra
```
"jogando" -> ["jog", "##ando"]
```
#### Byte-Level BPE
Semelhante ao Character-Level BPE, por√©m opera a n√≠vel de byte. 
- ***Usado pelos modelos de ML modernos: GPT, DeepSeek, LLaMA, RoBERTa***
- Palavras s√£o separadas em bytes que a medida que ocorre o treinamento cria-se tokens mais representativos.
- Opera em cima do UTF-8 e permite trabalhar com qualquer idioma, inclusive com emoji;  
- capaz de tratar ru√≠dos e erro de digita√ß√£o.  
```
Frase: "Ol√° üòä"
Convers√£o para bytes (UTF-8):
    "O" ‚Üí 0x4F (79)
    "l" ‚Üí 0x6C (108)
    "√°" ‚Üí 0xC3 0xA1 (dois bytes: 195, 161)
    espa√ßo ‚Üí 0x20 (32)
    "üòä" ‚Üí 0xF0 0x9F 0x98 0x8A (quatro bytes: 240, 159, 152, 138)
Sequ√™ncia de bytes: [79, 108, 195, 161, 32, 240, 159, 152, 138]
Depois do BPE: tokens como [79,108] ("Ol"), [195,161] ("√°"), [240,159,152,138] ("üòä"), etc.
```

## Implementa√ß√£o do Tokenizador BPE
Para implementar o Tokenizador BPE √© um processo que n√£o precisa de conex√£o com a internet. O objetivo √© construir um vocabul√°rio em forma de tokens e salvar para que possa processar qualquer texto de entrada. √â importante dizer que o total de tokens determina diretamente a efici√™ncia do BPE.
-Quanto maior o vocabul√°rio de tokens:  
    - Mais palavras representadas;  
    - Processamento mais r√°pido (Tokens s√£o mais precisos);  
    - Representa melhor idiomas extensos e ricos;  
    - **A etapa de embedding se torna mais pesada;**
    - **Mais mem√≥ria necess√°ria para processar o token, pois ser√° preciso quebrar em blocos;** 
    - **Risco de treinamento excessivo (overfitting)** 
        - Overfitting √© treinar al√©m da curva ideal, o que permite memorizar padr√µes errados, ru√≠do e dados irrelevantes.  
-Quanto menor o vocabul√°rio:  
    - Mais compacto;  
    - Treinamento r√°pido;  
    - Maior n√∫mero de tokens por palavra;  
    - Dificuldade em compreender palavras raras;  

### Pr√©-processamento
O BPE precisa de pouco pr√© processamento, pois analiza a n√≠vel de byte os textos. Mas dependendo do intuito pode ocorrer:
- Normaliza√ß√£o para o formato unicode: Remover acentos e caracteres especiais
- Remo√ß√£o de artefatos de c√≥digo: Tags HTML, XML e afins;
    - Dependendo do interesse ou do objetivo pode manter as tags
- N√£o existe a necessidade de o espa√ßo ou quebra de linha

### Convers√£o para Bytes
Todo o *corpus do texto* √© convertido em formato UTF-8 que pode ser subdividido em bytes;
- O [UTF-8](https://pt.wikipedia.org/wiki/UTF-8) possui por padr√£o 4bytes por caractere
    - 1byte para ASCII
    - 2 bytes se for idioma latino, podendo chegar a 3bytes para alguns idiomas
    - 1byte para marca√ß√µes  
- Em seguida √© separado em lotes de 4 bytes que podem assumir valor de 0 a 255;
```
Frase: "Ol√° üòä"
Convers√£o para bytes (UTF-8):
    "O" ‚Üí 0x4F (79)
    "l" ‚Üí 0x6C (108)
    "√°" ‚Üí 0xC3 0xA1 (dois bytes: 195, 161)
    espa√ßo ‚Üí 0x20 (32)
    "üòä" ‚Üí 0xF0 0x9F 0x98 0x8A (quatro bytes: 240, 159, 152, 138)
Sequ√™ncia para o BPE: [79, 108, 195, 161, 32, 240, 159, 152, 138]
```
- Ap√≥s criar a sequ√™ncia de tokens percorre todo o corpus do texto e conta o n√∫mero de repeti√ßoes de cada byte;
- Substitui os <ins>***2 blocos que se repetem juntos mais vezes***</ins> por um novo caractere e o adiciona no vocabul√°rio;  
- O processo se repete continuamente at√© que o total de tokens atinja o tamanho esperado; 

### Crit√©rios importantes para o BPA
#### Crit√©rio de parada
Um dos crit√©rios mais importantes pois o treinamento cont√≠nuo do BPE ir√° continuamente mesclar e reduzir o n√∫mero de tokens continuamente. Para isso limitamos o tamanho m√°ximo do dicion√°rio de tokens.
- Modelos de IA modernos usam  cerca de 100 a 128K tokens; 
    - DeepSeek: cerca de 128k com foco multilingual geral
    - GPT-3: cerca de 50k com foco em ingl√™s
    - GPT-4: cerca de 100k com foco multilingual geral
    - LLaMA 2: cerca de 32k com foco em ingl√™s
- Outro ponto importante √© que o computador opera em base 2 ent√£o ter como limite um n√∫mero em pot√™ncia de 2 ajuda na indexa√ß√£o
- Limite de mem√≥ria da GPU para processar os tokens, o idel √© usar paralelismo em servidores ou limitar o n√∫mero de lotes carregados na mem√≥ria da GPU;  

### Principais m√©tricas de vocabul√°rio
#### Taxa de OOV (Out of Value)
Representa o n√∫mero de palavras desconhecidas que n√£o est√£o representadas no tokenizador
```
OOV = (n√∫mero de palavras desconhecidas)/(total de tokens do tokenizador) * 100%
```
- O ideal √© ser o menor poss√≠vel se aproximando de 0

#### Cobertura de Caracteres Raros
- √â a capacidade de representar caracteres raros ou s√≠mbolos que s√£o poucos usados
- Evita que o modelo de IA apresente o s√≠mbolo ÔøΩÔøΩ para o usu√°rio
- O ideal √© que o corpus do texto usado para treinar o modelo do tokenizador apresente ***ao menos uma vez esses caracteres***
- Isso ocorre pois o tokenizador n√£o consegue representar todas as possibiliades poss√≠veis para 4 bytes.
    - 4 bytes permite representar ***cerca de 4.2 Bilh√µes de valores***
- O valor do CCR deve ser 0, ou seja representar todos os caracteres incomuns que existem para o idioma.

#### Tamanho do Vocabul√°rio x Cobertura
Representa a rela√ßao entre o vocabul√°rio e a cobertura, existe um limite para o quanto o aumento do vocabul√°rio gera de ganho, gerando uma curva exponencial.  
<img width="300" src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Binary_logarithm_plot_with_grid.png/1280px-Binary_logarithm_plot_with_grid.png">

O ideal √© a n√£o passar do limite do "Joelho" da curva, poiso  ganho ser√° muito pouco em rela√ß√£o ao tamanho.  

### N√∫mero de tokens por palavra
Representa a m√©dia de tokens gerados por palavra. √â dado por:
```
taxa = (total de tokens)/(total de palavras)
```  
Os principais valores para esse par√¢metro s√£o:
    - valores pr√≥ximo de 1 indica um vocabul√°rio muito grande, podendo haver overfitting  
    - pr√≥ximo de 2: O vocabul√°rio √© ineficiente, com muitas subpalavras  
    - **menor que 1,5: valor ideal**  
        - Estimasse que o total de tokens em portugu√äs usado nos modelos modernos esteja entre 1,2 e 1,5  

Para idiomas que n√£o usam palavras separadas por espa√ßo, pode usar ***Token por caractere***
```
taxa_caractere = (total de tokens)/(total de caracteres)
```

### N√∫mero de tokens √∫nicos
- Contagem de tokens que aparecem uma vez ou nenhuma vez;  
- Para isso usamos um texto representativo ou v√°rios textos representativos; 
- tokenizamos e contamos quantos tokens aparecem no corpus do texto e dividimos pelo total de tokens;  
- caso o valor seja muito alto h√° um superdimensionamento; 
    - Esperamos somente cerca de 95% de tokens sendo usados;

### Cobertura de n-grams
N-grams s√£o termos prontos muito usados, como do, de, um, inho, √ß√£o, em um;
- um tokenizador pouco representativo pode representar n-grams fragmentados, como inho = [i, nho]
- ter uma alta cobertura de n-grams permite evitar a fragmenta√ß√£o aumentando a velocidade de processamento;
- para isso verifica o total de n-grams tokens √∫nicos e o total de n-grams do idioma:
```
Cobertura = ((n-gram tokenizado)/(total de n-gram)) * 100
```
- o ideal √© a cobertura ser de 100%

### Entropia da distribui√ß√£o dos tokens
Mede a uniformidade de distribui√ß√£o dos tokens.
- Se houver poucos tokens voc√™ ir√° usar com muita frequ√™ncia, quase soletrando;  
- Enquanto um dicion√°rio muito extenso h√° muitos tokens pouco usados, tornando ineficiente e grande;
- O ideal √© ter poucas palavras muito repetidas, a maioria das palavras com uso m√©dio e tokens de palavras grandes por√©m muito √∫teis; 
- essa distribui√ß√£o se chama entropia:
    - √© a soma da frequ√™ncia de uso do token($p(token_i)$) multiplicado pelo logaritmo na base 2 de $p(token_i)$  

$$H = - \sum_{i} p(token_i) \log_2(p(token_i))$$

- Limites da entropia:
    - Uma entropia pr√≥ximo de 1 significa que o sistema √© ineficiente. H√° poucos tokens usados muitas vezes e a maioria dos tokens s√£o pouco usados;  
    - Uma entropia muito acima de 12 indica que os tokens s√£o usados de forma homg√™nea, provavelmente s√£o palavras muito grandes usadas poucas vezes de forma homg√™nea
    - Estima-se que a entropia ideal seja entre 7,5 e 10.
        - tem como base a Lei de Zipf, que descreve a distribui√ß√£o de frequ√™ncias em l√≠nguas naturais.
        - Os dados de entropia n√£o s√£o divulgados, mas rodando modelos antigos e abertos, se obtem uma entropia pr√≥xima de 10;  

### Estabilidade da segmenta√ß√£o
Verifica se a palavra est√° sendo quebrada no mesmo lugar, n√£o importando onde apare√ßa.
- Para isso criamos uma lista de segmenta√ß√µes poss√≠veis para a mesma palavra; 
- Selecionamos a segmenta√ß√£o mais comum como a correta (moda); 
- atribu√≠mos 1 sempre que ocorrer uma segmenta√ß√£o diferente da moda; 
- dividimos pelo total de ocorr√™ncias da palavra;
- A f√≥rmula, de uma palavra $w$ que aparece ${N_w}$ vezes no corpus, a vari√¢ncia da segmenta√ß√£o √©:
$$\DeclareMathOperator{Var}(w) = \frac{1}{N_w} \sum_{i=1}^{N_w} \mathbb{1}\left[ S_{w,i} \neq S_{w}^{\text{moda}} \right]$$
- Com isso calculamos a m√©dia ponderada:  
$$\text{Estabilidade Ponderada} = \frac{\sum_{w} N_w \times \text{Var}(w)}{\sum_{w} N_w}$$
- Ou a m√©dia normal
- O ideal √© a Estabilidade ser o mais pr√≥ximo de zero
### Reversibilidade
- √â a capacidade dos tokens gerarem palavras
- Basta pegar um texto tokenizar gerando os tokens como resultado;  
- Em seguida passar os tokens pelo Tokenizador. Ele deve retornar o texto original; 
- O ideal √© gerar o texto id√™ntico
### Robustez a ru√≠do
√â a capacidade de manter a integridade do texto, mesmo com erro de digita√ß√£o.
- Para isso introduz um erro e verifica os tokens gerados
- Ex `A casa √© bonita`trocando casa por caza:
    - A vers√£o original gerou 4 tokens: `[A, casa, √©, bonita]`;
    - O erro gerou 4 tokens: `[A, caza, √©, bonita]`;
    - Foi alterado um token em 5 `1/4 = 25%`;
    - Agora se gerar 5 tokens `[A, ca, za, √©, bonita]` 
        - Foi alterado 2 tokens logo `2/5 = 40%`;
- Esperamos uma altera√ß√£o m√≠nima de tokens.
