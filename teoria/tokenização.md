# TokenizaÃ§Ã£o
Essa tÃ©cnica surgiu a partir de uma necessidade bÃ¡sica: computadores nÃ£o conseguem reconhecer palavras. EntÃ£o para que possamos transmitir um texto para o computador compreender precisamos criar tokens numÃ©ricos, onde Ã© possÃ­vel aplicar cÃ¡lculos matemÃ¡ticos e inferir dados usando estatÃ­sticas ou Ã¡lgebra linear.  
```
Texto = "OlÃ¡, mundo!"
Tokens = ["OlÃ¡", ",", "mundo", "!"]
Tokens = [15678, 11, 2345, 30]
```
Pontos importantes sobre a tokenizaÃ§Ã£o:  
- **Uma palavra** nÃ£o necessÃ¡riamente vai gerar **um token**;  
- Palavras compostas ou muito grandes podem gerar mais de um token;  
- Existem tokens especiais de marcaÃ§Ã£o para inferÃªncia, como a marcaÃ§Ã£o de inÃ­cio e fim de texto, pedido do usuario, resposta do modelo entre outros.  
- O token gerado depende do contexto onde estÃ¡ a palavra e isso depende de muito treinamento.  
- Quando fornecemos **palavras desconhecidas** o tokenizador quebra a palavra em trechos com tokens conhecidos.  

## TÃ©cnicas de tokenizaÃ§Ã£o
### TokenizaÃ§Ã£o por palavras (Word Based)
A forma mais intuitiva de tokenizaÃ§Ã£o, onde divide o texto por palavra e pontuaÃ§Ã£o.  
- NÃ£o consegue lidar com palavras desconhecidas;  
- Pode ou nÃ£o ignorar pontuaÃ§Ã£o;  
```
"Eu amo inteligÃªncia artificial" -> ["Eu", "amo", "inteligÃªncia", "artificial", "!"]
```
### TokenizaÃ§Ã£o por caracteres (Character-Based)
Divide o texto em caracteres individuais.  
- Usado apenas para gerar poucas centenas de tokens;  
- capaz de lidar com todas as palavras;  
- quanto maior o texto, mais difÃ­cil o processamento;  
```
"casa" -> ["c", "a", "s", "a"]
```
### TokenizaÃ§Ã£o MorfolÃ³gica  (Morphological)
Usa as regras linguÃ­sticas para dividir as palavras.
- Extremamente preciso
- Precisa de um conhecimento profundo do idioma
- Restrito a um idioma
- DifÃ­cil de generalizar para outros idiomas
```
"inesperadamente" â†’ ["in", "esper", "ada", "mente"]
```

### TokenizaÃ§Ã£o por Subpalavras (Subword Tokenization)
Busca o equilÃ­brio entre tokenizaÃ§Ã£o por palavra e por caractere. Possuem vÃ¡rias formas de aplicaÃ§Ã£o:  
#### Character-Level Encoding 
- Converte os caracteres para unicode;  
- ComeÃ§a separando caracteres individuais e vai mesclando os caracteres a medida que verifica a frequÃŠncia;
- Exige muito treinamento
- Consegue trabalhar com subpalavras e palavras desconhecidas
```
"baixo" (antes do treino ou palavra rara) -> ["b", "a", "i", "x", "o"]
"baixo" (apÃ³s treino) -> ["baix", "o"] ou ["baixo"] se for uma palavra comum
```

#### WordPiece
Semelhante ao BPE mas usa anÃ¡lise estatÃ­stica para gerar os tokens.  
- Usado por BERT, DistilBERT
- Usa o sÃ­mbolo `##` para indicar continuaÃ§Ã£o de palavra
```
"jogando" -> ["jog", "##ando"]
```
#### Byte-Level BPE
Semelhante ao Character-Level BPE, porÃ©m opera a nÃ­vel de byte. 
- ***Usado pelos modelos de ML modernos: GPT, DeepSeek, LLaMA, RoBERTa***
- Palavras sÃ£o separadas em bytes que a medida que ocorre o treinamento cria-se tokens mais representativos.
- Opera em cima do UTF-8 e permite trabalhar com qualquer idioma, inclusive com emoji;  
- capaz de tratar ruÃ­dos e erro de digitaÃ§Ã£o.  
```
Frase: "OlÃ¡ ğŸ˜Š"
ConversÃ£o para bytes (UTF-8):
    "O" â†’ 0x4F (79)
    "l" â†’ 0x6C (108)
    "Ã¡" â†’ 0xC3 0xA1 (dois bytes: 195, 161)
    espaÃ§o â†’ 0x20 (32)
    "ğŸ˜Š" â†’ 0xF0 0x9F 0x98 0x8A (quatro bytes: 240, 159, 152, 138)
SequÃªncia de bytes: [79, 108, 195, 161, 32, 240, 159, 152, 138]
Depois do BPE: tokens como [79,108] ("Ol"), [195,161] ("Ã¡"), [240,159,152,138] ("ğŸ˜Š"), etc.
```