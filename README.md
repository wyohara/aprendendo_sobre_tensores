# Aprendendo sobre Tensores üß†üìä

Bem-vindo ao reposit√≥rio **Aprendendo sobre Tensores**! Este projeto tem como objetivo desmistificar o funcionamento de redes neurais profundas atrav√©s do estudo e manipula√ß√£o de tensores, a estrutura de dados fundamental para o deep learning.

O conte√∫do did√°tico est√° organizado em t√≥picos progressivos, cada um com seu pr√≥prio arquivo, facilitando o aprendizado passo a passo.

## üìö Sobre o Projeto

Aqui voc√™ encontrar√° explica√ß√µes claras e exemplos pr√°ticos sobre:

- O que s√£o tensores e por que eles s√£o essenciais em redes neurais
- Opera√ß√µes b√°sicas e avan√ßadas com tensores
- Constru√ß√£o de camadas de redes neurais usando tensores
- Implementa√ß√£o de um modelo simples do zero (sem frameworks de alto n√≠vel)

Todo o c√≥digo √© escrito em **Python**, utilizando bibliotecas de c√≥digo aberto, e √© mantido de forma aberta para que qualquer pessoa possa estudar, modificar e reutilizar.


## Etapas do processamento de texto
### 1Ô∏è‚É£ Tokeniza√ß√£o
- O texto √© dividido em unidades menores chamadas tokens (palavras, partes de palavras ou caracteres)  
    - Cada token √© convertido em um ID num√©rico que o modelo consegue processar  
    - Exemplo: "Ol√°, como voc√™ est√°?" ‚Üí [243, 567, 8910, ...]
    - Mais informa√ß√µes [aqui](/teoria/tokeniza√ß√£o.md#tokeniza√ß√£o).
### 2Ô∏è‚É£ Embedding
- Os tokens s√£o transformados em vetores num√©ricos (embeddings)  
    - Esses vetores capturam significados sem√¢nticos e rela√ß√µes entre palavras  
        - Usa como base o [teorema de Manifold](/teoria/tensores.md#teorema-de-manifold) que permite usar as dimens√µes para reduzir o tamanho total.
    - Tokens semelhantes ficam mais pr√≥ximos no espa√ßo vetorial
        - O processo pode ser visto com detalhes [aqui](/teoria/tensores.md#entendendo-tensores)    
### 3Ô∏è‚É£ Processamento pelo Transformer
- O modelo aplica m√∫ltiplas camadas de aten√ß√£o para entender o contexto
    - A arquitetura Transformer analisa rela√ß√µes entre todas as palavras da frase simultaneamente
    - O mecanismo de autoaten√ß√£o identifica quais palavras s√£o mais relevantes para cada parte do texto
### 4Ô∏è‚É£ Compreens√£o Contextual
- O modelo refina iterativamente sua representa√ß√£o do texto
    - Cada camada adiciona mais contexto e nuances √† compreens√£o
    - Informa√ß√µes sobre sintaxe, sem√¢ntica e inten√ß√£o s√£o extra√≠das
### 5Ô∏è‚É£ Gera√ß√£o Autoregressiva
- O modelo come√ßa a gerar a resposta token por token
    - Para cada novo token, considera todo o contexto anterior (sua pergunta + o que j√° gerou)
    - Usa probabilidades para escolher o pr√≥ximo token mais adequado
### 6Ô∏è‚É£ Decodifica√ß√£o
- Estrat√©gias como amostragem ou beam search s√£o usadas para selecionar tokens
    - Par√¢metros como temperatura controlam a criatividade vs. determinismo
    - O modelo pode reavaliar escolhas anteriores durante a gera√ß√£o
### 7Ô∏è‚É£ P√≥s-processamento
- Os tokens gerados s√£o convertidos de volta para texto leg√≠vel
    - Pequenos ajustes podem ser aplicados (formata√ß√£o, remo√ß√£o de tokens especiais)

### 8Ô∏è‚É£ Entrega da Resposta
- O texto final √© apresentado a voc√™ de forma organizada e coerente