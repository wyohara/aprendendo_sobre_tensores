# Aprendendo sobre Tensores üß†üìä

Bem-vindo ao reposit√≥rio **Aprendendo sobre Tensores**! Este projeto tem como objetivo desmistificar o funcionamento de redes neurais profundas atrav√©s do estudo e manipula√ß√£o de tensores, a estrutura de dados fundamental para o deep learning.

O conte√∫do did√°tico est√° organizado em t√≥picos progressivos, cada um com seu pr√≥prio arquivo, facilitando o aprendizado passo a passo.

## üìö Sobre o Projeto

Aqui voc√™ encontrar√° explica√ß√µes claras e exemplos pr√°ticos sobre:

- O que s√£o tensores e por que eles s√£o essenciais em redes neurais
- Opera√ß√µes b√°sicas e avan√ßadas com tensores
- Constru√ß√£o de camadas de redes neurais usando tensores
- Implementa√ß√£o de um modelo simples do zero (sem frameworks de alto n√≠vel)

Todo o c√≥digo √© escrito em **Python**, utilizando bibliotecas de c√≥digo aberto, e √© mantido de forma aberta para que qualquer pessoa possa estudar, modificar e reutilizar.


# Entendendo tensores
Quando pensando em n√∫meros podemos organizar em dimens√µes:  
1. ***Escalar:*** √© um n√∫mero por si s√≥.  
    - Ex.: $25$;  
2. ***Vetor***: √© uma lista de n√∫meros.  
    - Ex: $[5, 3, 6, 35]$  
    - Vetores possuem apenas 1 dimens√£o  
3. ***Matriz***: √© uma tabela de n√∫meros com duas coordenadas, a *linha* e a *coluna*.  
    - Ex: $[[3, 5], [8, 12]]$
    - Matrizes possuem apenas 2 dimens√µes  
4. ***Tensores***: √© uma generaliza√ß√£o com 3 ou mais dimens√µes.  


## Por qual motivo usamos tensores?
Computadores operam com n√∫meros, n√£o com palavras. Por isso, antes de pensar em trabalhar com um modelo de linguagem com um texto, n√≥s precisamos converter palavras em representa√ß√µes num√©ricas. Mas n√£o podemos converter diretamente cada palavra para um n√∫mero - isso resultaria em um vetor gigantesco e esparso.  
Para contornar essa limita√ß√£o, vamos fazer uma representa√ß√£o compacta focada em manter as rela√ß√µes entre as palavras do texto.  Isso s√≥ √© poss√≠vel usando o [Teorema de Manifold](#teorema-de-manifold), que traz a ideia de que dados extensos podem ser compactados a medida que reduzimos as dimens√µes.  

Vamos pensar nas palavras gato, cachorro, le√£o, carro e caneta em um tensor de 3 dimens√µes:
```
gato: (1, 0, 0.3) # animal, pequeno, pouco perigoso    
cachorro: (1, 0.4, 0.4) # animal, m√©dio, m√©dio perigoso
le√£o: (1, 1, 1) #animal, grande, perigoso
carro: (0, 1, 0.2) # n√£o animal, grande, pouco perigoso    
caneta: (0, 0, 0) # n√£o animal, pequeno, inofensivo
```
Perceba que:    
- Palavras com significado pr√≥ximo (como gato e cachorro) t√™m vetores parecidos.
- Palavras distintas (caneta e le√£o) ficam distantes nesse espa√ßo tridimensional.  

Na pr√°tica a cria√ß√£o dessas rela√ß√µes n√£o √© feita manualmente, mas sim computacionalmente por treinamento (embeding). Quanto a semelhan√ßa, computacionalmente e matematicamente poderiamos calcular a semelhan√ßa usando √°lgebra linear, principalmente pela ***regra dos cossenos***.  

### Teorema de Manifold
> "Pense em uma folha de papel A4, ela esticada ir√° ocupar apenas duas dimens√µes, o comprimento(eixo X) e a largura (eixo Y) e uma altura quase desprez√≠vel (eixo Z). Agora se amassarmos a folha de papel em uma bola ela ir√° ter um comprimento  e larguras menores, por√©m ir√° ter uma nova dimens√£o importante, a <ins>*altura*</ins>."  

Tendo essa premissa em mente se criarmos novas dimens√µes de an√°lise podemos reduzir o tamanho de um texto para um tamanho muito menor, e essas novas dimens√µes ir√£o guardar apenas as rela√ß√µes entre as palavras do texto original. 
#### Aplica√ß√£o do teorema de manifold
- Um exemplo de aplica√ß√£o do teorema de manifold √© o RGB.  
    - O RGB pode ser visto como um tensor de 3 dimens√µes, cada dimens√£o representando uma cor: Vermelho, Verde e Azul.  
    - Cada cor a partir do RGB pode ser definida com uma quantidade de vermelho, verde e azul assim usando apenas 3bytes, um para cada posi√ß√£o do Tensor.  
    - Parece pouco mas como cada cor vai de 0 a 255 teremos cerca de 16,7 milh√µes de cores:
    $$ 256 \cdot 256 \cdot 256 \approx 16,7 M$$

#### Manifold aplicado no texto
- O vocabul√°rio portugu√™s tem cerca de 380000 palvras e para analisar um texto de 30000 palavras (cerca de 100 p√°ginas):
    - Para analisar a rela√ß√£o das palavras vamos criar uma lista $V$ com 380000 posi√ß√µes. 
        - Nessa lista todos os valores s√£o 0 e somente a posi√ß√£o da palavra ter√° um n√∫mero que representa a sua rela√ß√£o com as palavras em volta:
        $$4bytes \cdot 380000 \approx 1,5 MB$$
        - Como o texto tem 30000 palavras cada linha ser√° a posi√ß√£o da palava no texto. Ex: primeira palavra na linha um, segunda palavra na linha 2 e assim segue...
        - E cada coluna ser√° o valor da palavra na lista $V$. Assim geramos uma matriz de 30000 linhas e 380000 colunas
        - O tamanho dessa nova matriz ser√°:
        $$1,5MB \cdot 30000 = 45,6GB$$
        - Assim a matriz de entrada ter√° 45,6 Gb de tamanho!. √â imenso!
- Agora no mesmo se usarmos um tensor de 500 dimens√µes  
    - um tensor $T$ de 500 dimens√µes com 4 bytes em cada posi√ß√£o:
    $$500*4 = 2Kb$$
    - como temos 38000 palavras no vocabul√°rio iremos ter 380000 tensores $T$:
    $$2Kb \cdot 380000 = 760 Mb$$
    - Assim a matriz de vocabul√°rio ter√° 760 Mb para todas as palavras!
    - se quisermos analisar o texto de 30000 palavras basta pegar o tensor T que corresponde a palavra:
    $$ 2Kb \cdot 30000=60Mb$$
    - Assim, para representar o mesmo texto precisaremos de apenas $60Mb$. ***Uma redu√ß√£o de quase 760 vezes!***
